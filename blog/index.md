---
layout: post
title: Mini Research Projects
---


## [[Work-In-Progress] Aggregate Exchange Models for Approximation](https://tchlux.github.io/blog/2022-02_aggregate-exchange-model/)

A take on capturing the approximation power of transformers in a much simpler architecture. It's a work-in-progress for now, but will keep adding as things are refined.


## [Controlling bias with data from other problems](https://tchlux.github.io/blog/2022-02_smoothing-with-artificial-data/)

An alternative to pretraining that guarantees a model captures some desirable underlying structure for problems where very little specific data is available, but data for a similar domain is available in abundance.


## [MLP's are actually nonlinear âžž linear preconditioners](https://github.com/tchlux.github.io/blog/2021-10_mlp_nonlinear_linear_preconditioner/)

Sometimes it's easy to overlook the fundamental nature of neural networks with all their complexity. Almost all architectures reduce to performing linear regression in the last layer, but that's more useful than it may seem.


[//]: # ## [Mechanical Groundwork](https://tchlux.github.io/blog/2022-02_getting_started/)
[//]: # Testing out the mechanics of posting to this site. How do things look? Does a multiline summary beneath the posts make sense right here? Only one way to find out.
