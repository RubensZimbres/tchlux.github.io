---
layout: post
title: Research Projects by `tlux`
---

# Research Projects by `tlux`
[![DOI](https://zenodo.org/badge/221336331.svg)](https://zenodo.org/badge/latestdoi/221336331)

This blog tracks my active and past research projects. The purpose is twofold: (1) to do good science that is open to the community, and (2) forcing me to formalize, clarify, and polish my thoughts regularly. It's my hope that all work here is complete and correct (unless labeled as in-progress). If you find any problems or errors, please don't hesitate to [raise an issue](https://github.com/tchlux/tchlux.github.io/issues)!

Cite this research blog in BibTex with
```
@book{tchlux:research,
  title     = "Research Compendium",
  author    = "Lux, Thomas C.H.",
  year      = 2022,
  publisher = "Github Pages",
  doi       = "10.5281/zenodo.6071692",
  url       = "https://tchlux.info/research"
}
```

Or cite any specific posts with
```
@incollection{tchlux:research,
  title     = "<post-title>",
  booktitle = "Research Compendium",
  author    = "Lux, Thomas C.H.",
  year      = 2022,
  publisher = "Github Pages",
  doi       = "10.5281/zenodo.6071692",
  url       = "https://tchlux.info/research"
}
```

# Articles with Summaries

## [Aggregate Exchange Models for Approximation](https://tchlux.github.io/research/2022-02_aggregate-exchange-model/)

A take on capturing the approximation power of transformers in a much simpler architecture. It's a **work-in-progress**, will keep adding as things are refined.


## [Controlling bias with data from other problems](https://tchlux.github.io/research/2022-02_smoothing-with-artificial-data/)

An alternative to pretraining that guarantees a model captures some desirable underlying structure for problems where very little specific data is available, but data for a similar domain is available in abundance.


## [MLP's are actually nonlinear âžž linear preconditioners](https://tchlux.github.io/research/2021-10_mlp_nonlinear_linear_preconditioner/)

Sometimes it's easy to overlook the fundamental nature of neural networks with all their complexity. Almost all architectures reduce to performing linear regression in the last layer, and that's more useful than it may seem.


[//]: # ## [Mechanical Groundwork](https://tchlux.github.io/research/2022-02_getting_started/)
[//]: # Testing out the mechanics of posting to this site. How do things look? Does a multiline summary beneath the posts make sense right here? Only one way to find out.
